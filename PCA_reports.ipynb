{"cells":[{"cell_type":"markdown","metadata":{"id":"mBkTqCaxDanC"},"source":["### Dimensionality Reduction"]},{"cell_type":"markdown","metadata":{"id":"LTH4kGu-DanJ"},"source":["Giảm chiều dữ liệu là một trong những kỹ thuật quan trọng của Machine Learning. Các feature trong các bài toán thực tế có số chiều lớn, tới vài nghìn. Ngoài ra, số điểm dữ liệu cũng thường rất lớn. Nếu thực hiện lưu trữ, thực hiện các mô hình học máy thì sẽ gặp khó khăn về cả việc lưu trữ và tốc độ tính toán.\n","\n","Dimensionality Reduction, một cách đơn giản là ta đi tìm một ánh xạ, với đầu vào là điểm dữ liệu $x_i \\in \\mathbb{R}^D$ với $D$ rất lớn, tạo ra một điểm dữ liệu mới $z \\in \\mathbb{R}^K (K < D)$\n","\n","Một thuật toán cơ bản nhất trong Dimensionality Reduction dựa trên một mô hình tuyến tính. Phương pháp này có tên là **Principal Compoment Analysis (PCA)**, tức Phân tích thành phần chính.\n","\n","Phương pháp này dựa trên quan sát rằng dữ liệu thường không phân bố ngẫu nhiên trong không gian mà thường phân bố gần các đường hoặc các mặt đặc biệt nào đó.\n","\n","PCA chính là việc đi tìm một hệ cơ sở mới sao cho thông tin của dữ liệu chủ yếu tập trung ở một vài tọa độ, phần còn lại chỉ mang lượng nhỏ thông tin có thể chấp nhận bỏ đi được. Và để đơn giản cho trong tính toán. PCA sẽ đi tìm một hệ trực chuẩn để làm cơ sở mới.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n0kyIMiZDanL"},"source":["### Phân tích toán học\n","\n","Như đã đề cập ở trên. PCA sẽ đi tìm một hệ cơ sở trực chuẩn để làm cơ sở mới. Vậy, ta giả sử hệ cơ sở trực chuẩn mới là $U$ và chúng ta muốn giữ lại $K$ tọa độ trong hệ cơ sở mới. Không mất tổng quát, giả sử đó là $K$ thành phần đầu tiên.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"pLDgskMhDanM"},"source":["Hình trên cho thấy hệ cơ sở mới $U = [U_K, \\overline{U}_K]$. $U$ là một hệ trực chuẩn $U_K$ với ma trận được tạo bởi $K$ cột đầu tiên của U. Với cơ sở mới này, dữ liệu có thể được viết thành.\n","\n","$$X = U_K Z + \\bar{U}_K Y \\quad (1)$$ \n","\n","Ta cũng có thể viết dưới dạng sau\n","\n","$$ X = \\begin{bmatrix} U_K & \\bar{U_k} \\end{bmatrix} \\begin{bmatrix} Z \\\\ Y \\end{bmatrix} $$\n","\n","$$-> \\begin{matrix} U_k^TX = Z \\\\ \\bar{U}_K^TX = Y \\end{matrix} \\qquad (2) $$"]},{"cell_type":"markdown","metadata":{"id":"uTIP5HXrDanN"},"source":["Chúng ta cố gắng giữ lại nhiều thông tin nhất có thể ở phần $U_KZ$ và lược bỏ phần $\\bar{U_K}Y$. Để phần lược bỏ đi không phụ thuộc vào từng điểm dữ liệu. Ta sẽ xấp xỉ $Y$ bởi một ma trận có toàn bộ các cột là như nhau. Gọi mỗi cột đó là $b$ và có thể coi nó là bias. Khi đó, ta sẽ xấp xỉ:\n","$$Y \\approx b \\mathbb{1}^T$$\n","\n","Trong đó $\\mathbb{1}^T \\in \\mathbb{R}^{1 \\times N}$ là ccasc hàng có toàn bộ phần tử bằng 1. Giả sử tìm được $U$, ta cần tìm $b$ thỏa mãn:\n","$$b = argmin \\|Y - b \\mathbb{1}^T \\|_F^2 = argmin \\| \\bar{U}_K^TX - b\\mathbb{1}^T \\|_F^2$$\n","\n","Giải phương trình trên theo đạo hàm của b với hàm mục tiêu:\n","$$-> b = \\bar{U}_K^T\\bar{x}$$\n","\n","Việc tính toán sẽ thuận tiện nếu vecto kỳ vọng $\\bar{x} = 0$. Việc này có thể thực hiện ngay từ đầu (Trừ mỗi vector cho kỳ vọng của toàn bộ dữ liệu), bước đầu tiên của PCA.\n","\n","Với giá trị b tìm được, dữ liệu ban đầu được xấp xỉ với:\n","\n","$$X \\approx \\widetilde{X} = U_kZ + \\bar{U}_k\\bar{U}_K^T\\bar{x}\\mathbb{1}^T \\qquad (3) $$"]},{"cell_type":"markdown","metadata":{"id":"auAcABDpDanO"},"source":["Kết hợp $(1)$ $(2)$ $(3)$ ta định nghĩa hàm mất mát như sau:\n","$$ J = \\frac{1}{N} \\|X - \\widetilde{X} \\|_F^2 = \\frac{1}{N} \\|\\bar{U}_K\\bar{U}_K^TX - \\bar{U}_K\\bar{U}_K^T\\bar{x}\\mathbb{1}^T\\|_F^2 \\quad (4)$$\n","\n","Hàm $(4)$ có thể được viết lại thành:\n","\n","$$J = \\frac{1}{N} \\sum_{i = K + 1}^{D} \\|\\hat{X}^Tu_i\\|_2^2$$\n","\n","$$ = \\frac{1}{N} \\sum_{i = K + 1}^{D} u_i^T\\hat{X}\\hat{X}^Tu_i$$\n","$$ = \\sum_{i = K + 1}^{D} u_i^T\\hat{X}\\hat{X}^Tu_i$$\n","\n","$$ = \\sum_{i = K + 1}^{D} u_i^TSu_i \\quad (5)$$\n","\n","Với $\\hat{X} = X - x\\mathbb{1}^T$ là dữ liệu chuẩn hóa và $S = \\frac{1}{N}\\hat{X}\\hat{X}^T $ là ma trận hiệp phương sai. S là ma trận bán xác định dương."]},{"cell_type":"markdown","metadata":{"id":"PfGyxwdGDanP"},"source":["Với K bằng 0, ta có:\n","$$ L = \\sum_{i = 1}^Du_i^TSu_i = \\frac{1}{N}\\|\\hat{X}^TU\\|_F^2 = \\frac{1}{N}trace(\\hat{X}^TUU^T\\hat{X}) $$\n","\n","$$ = \\frac{1}{N}trace(\\hat{X}^T\\hat{X}) = \\frac{1}{N}trace(\\hat{X}\\hat{X}^T) = trace(S) = \\sum_{i=1}^{D} \\lambda_i \\quad (6)$$"]},{"cell_type":"markdown","metadata":{"id":"SpCy9HXxDanP"},"source":["Ở đây $\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_i$ là các trị riêng của ma trận xác định dương $S$. Chú ý rằng các giá trị riêng này là các phần tử không âm. \n","\n","**Như vậy $L$ không phụ thuộc vào hệ cở sở trực giao U** và bằng tổng các phần tử trên đường chéo của $S$. Nói cách khác, $L$ chính là tổng các phương sai của từng thành phần dữ liệu ban đầu. \n","\n","Vì vậy, hàm mát J được cho bởi $(5)$ tương đương với việc tối đa:\n","\n","$$F = L - J = \\sum_{i = 1}^{K}u_i^TSu_i$$\n","\n","**Định lý 1:** *F* đạt giá trị lớn nhất bằng $\\sum_{i=1}^{K}\\lambda_i$ khi $u_i$ là các vector riêng ${u_i}$ ứng với giá trị riêng ${\\lambda_i}$ lập thành một hệ trực chuẩn: ${u_i}$ trực giao và $\\|u_i\\|_2 = 1$, $i = 1, ..., k$"]},{"cell_type":"markdown","metadata":{"id":"mS0P8aaDDanQ"},"source":["## Các bước PCA\n","\n","1. Tính kỳ vọng của toàn bộ dữ liệu \n","\n","$$\\hat{x} = \\frac{1}{N}\\sum_{n = 1}^{N}x_n $$\n","\n","2. Tính dữ liệu chuẩn hóa $\\hat{x}_n$\n","\n","$$\\hat{x}_n = x_n - \\bar{x} \\quad n = 1, 2, ..., N.$$\n","\n","3. Tính ma trận hiệp phương sai\n","\n","$$S = \\frac{1}{N}\\hat{X}\\hat{X}^T$$\n","\n","4. Tính các giá trị riêng $\\lambda_i$ và vector riêng $u_i$ có \\|u_i\\|_2 = 1 của ma trận này, sắp xếp chúng theo thứ tự giảm dần của giá trị riêng.\n","\n","5. Chọn $k$ giá trị riêng lớn nhất và $k$ vector riêng trực chuẩn tương ứng. Xây dựng ma trận $U_k$\n","\n","6. Các cột của ${U_i}_{i=1}^{k}$ là hệ cơ sở trực chuẩn, tạo thành không gian con của k thành phần chính, gần với phân bố của dữ liệu ban đầu đã chuẩn hóa.\n","\n","7. Chiếu dữ liệu ban đầu đã chuẩn hóa \\hat{X} xuống không gian con nói trên. Dữ liệu mới chính là tọa độ của các điểm dữ liệu trên không gian mới\n","\n","$$Z = U_k^T\\hat{X}$$\n"]},{"cell_type":"markdown","metadata":{"id":"fd954fvsDanQ"},"source":["Vì bước tính toán giá trị riêng khá phức tạp, vậy chúng ta sẽ sử dụng thư viện numpy để tính các giá trị riêng, và hệ vecto bằng khai triển SVD"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}